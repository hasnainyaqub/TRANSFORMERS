# Understanding Transformers with Practical Coding

This folder contains practical coding examples that help you understand how transformer models work in real applications. You will find demonstrations of Hugging Face pipelines, token classification tasks, named entity recognition with BERT, and fine tuning GPT models on custom datasets.

---

## Contents of This Folder

### 1. Hugging Face Pipelines
Simple examples showing how to run common NLP tasks using easy to use pipelines.  
Tasks include sentiment analysis, text generation, translation, summarization, and token classification.

### 2. NER with BERT
Named Entity Recognition code using a pretrained BERT model.  
You will learn how to identify people, locations, organizations, and other entity types from text.

### 3. GPT Fine Tuning
Examples that show how to fine tune GPT models for custom generation tasks.  
Includes dataset loading, tokenization, training, evaluation, and text generation.

---

## What You Will Learn

- How to apply transformer models in real coding environments  
- How Hugging Face pipelines simplify NLP tasks  
- How to perform NER using BERT  
- How to fine tune GPT for domain specific text generation  
- How to structure datasets for transformer training  
- How to run inference and evaluate outputs  

---

## Useful References

Hugging Face Transformers documentation  
https://huggingface.co/docs/transformers

BERT paper  
https://arxiv.org/abs/1810.04805

GPT2 paper  
https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf
