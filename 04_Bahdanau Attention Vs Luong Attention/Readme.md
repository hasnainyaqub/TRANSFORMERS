# Bahdanau Attention vs. Luong Attention

This note compares two prominent attention mechanisms used in sequence-to-sequence models: Bahdanau (additive) and Luong (multiplicative) attention.

## ğŸ”— Research Papers  
- Bahdanau Attention: â€œNeural Machine Translation by Jointly Learning to Align and Translateâ€ (https://arxiv.org/abs/1409.0473) :contentReference[oaicite:2]{index=2}  
- Luong Attention: â€œEffective Approaches to Attention-based Neural Machine Translationâ€ (https://arxiv.org/abs/1508.04025) :contentReference[oaicite:3]{index=3}  

## ğŸ§  What Each Mechanism Does  
- **Bahdanau Attention (Additive)**: The decoder computes alignment scores based on the decoderâ€™s previous hidden state and each encoder hidden state via a feed-forward network. :contentReference[oaicite:4]{index=4}  
- **Luong Attention (Multiplicative / Dot-Product / General)**: The decoder uses its current hidden state and the encoder hidden states in a simpler scoring function (dot-product, general, or concat) and attends globally or locally. :contentReference[oaicite:5]{index=5}  

## ğŸ“Š Key Differences  
| Feature                          | Bahdanau (2014)                               | Luong (2015)                                  |
|----------------------------------|-----------------------------------------------|-----------------------------------------------|
| Score function                   | Additive (feed-forward)                        | Dot-product, General, Concat                   |
| Uses decoder **previous** state | âœ”                                            | âœ– (uses current state)                         |
| Attention scope                  | Implicitly global                              | Global and Local variants                      |
| Complexity & speed               | Slightly heavier                              | Simpler & faster in many cases                 |
| Typical use-case                | Robust alignment for long input sequences     | Efficient attention, often used in practise   :contentReference[oaicite:6]{index=6}  

## ğŸ›  Why It Matters  
Choosing the right attention mechanism affects model performance and complexity.  
- Bahdanauâ€™s method helps with very long sequences by focusing adaptively.  
- Luongâ€™s method introduces efficiency and variant choices (global vs local) which may fit production settings better. :contentReference[oaicite:7]{index=7}  

## ğŸ“š Further Reading  
- For implementation details and code examples, check tutorials on Bahdanau vs Luong attention.  
- For a deeper dive into attention mechanisms, see also the broader â€œAttention Is All You Needâ€ paper (2017) which extends these ideas to the Transformer architecture. :contentReference[oaicite:8]{index=8}  

## âœ… Summary  
Both mechanisms are foundational in modern NLP.  
Use Bahdanau for flexible alignment in complex settings; use Luong for efficient and varied alignment schemes in production-oriented models.

